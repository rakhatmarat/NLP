{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ae8aa3",
   "metadata": {},
   "source": [
    "# Text Classification using Logistic Regression — SOLVED\n",
    "\n",
    "This notebook provides a complete, **offline-friendly** solution for binary sentiment classification using scikit‑learn's **LogisticRegression**. It tries to load `twitter-sa.csv` from the working directory; if not found, it falls back to a small in‑notebook dataset so you can run everything without downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0164d7",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7115ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For saving artifacts (optional)\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de9c79",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "The original tutorial uses Sentiment140 (`twitter-sa.csv`). We will attempt to read it. If it's not available, we'll generate a small labeled dataset inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8868db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_or_create_dataset(path='twitter-sa.csv', sample_size=None):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        # Dataset columns in many Sentiment140 dumps: target, id, date, flag, user, text\n",
    "        df = pd.read_csv(p, encoding='ISO-8859-1', header=None,\n",
    "                         names=['target','id','date','flag','user','text'])\n",
    "        # keep only class and text\n",
    "        df = df[['target','text']].dropna()\n",
    "        # Normalize target to {0,1}\n",
    "        # In Sentiment140: 0=negative, 4=positive\n",
    "        df['target'] = (df['target'] == 4).astype(int)\n",
    "        if sample_size is not None:\n",
    "            df = df.sample(min(sample_size, len(df)), random_state=42).reset_index(drop=True)\n",
    "        return df\n",
    "    else:\n",
    "        # Fallback mini dataset (balanced, 80 examples)\n",
    "        pos = [\n",
    "            \"I love this movie so much!\", \"What a wonderful day!\", \"Best service ever, highly recommend.\",\n",
    "            \"Absolutely fantastic experience.\", \"Great vibes, feeling amazing!\", \"This is awesome!\",\n",
    "            \"I am very happy with the results.\", \"The product works perfectly.\", \"Such a pleasant surprise.\",\n",
    "            \"Everything went smoothly and I’m satisfied.\", \"Brilliant performance!\", \"The food was delicious.\",\n",
    "            \"So proud of myself today.\", \"I like this approach a lot.\", \"Smiling all day long.\",\n",
    "            \"Superb quality and great support.\", \"The lecture was inspiring!\", \"Thank you, I'm grateful.\",\n",
    "            \"I enjoyed every minute of it.\", \"This app is very helpful.\"\n",
    "        ]\n",
    "        neg = [\n",
    "            \"I hate this, worst experience.\", \"Terrible service and rude staff.\", \"Absolutely disappointed.\",\n",
    "            \"This is awful!\", \"I'm very unhappy with the product.\", \"What a waste of time.\",\n",
    "            \"Nothing works as expected.\", \"So frustrating and annoying.\", \"Bad quality, not worth it.\",\n",
    "            \"Everything went wrong... again.\", \"Horrible performance.\", \"The food was disgusting.\",\n",
    "            \"I feel sad and exhausted.\", \"I don't like this at all.\", \"Crying all night.\",\n",
    "            \"Poor quality and no support.\", \"The lecture was boring.\", \"This is unacceptable.\",\n",
    "            \"I regret buying this.\", \"The app is useless.\"\n",
    "        ]\n",
    "        df = pd.DataFrame({\n",
    "            'text': pos + neg,\n",
    "            'target': [1]*len(pos) + [0]*len(neg)\n",
    "        })\n",
    "        return df\n",
    "\n",
    "df = load_or_create_dataset(sample_size=50000)  # sample if the full csv is large\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Class balance:\\n\", df['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cb6a5",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "We'll implement a light text cleaner and use `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a498f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Precompile regexes for speed\n",
    "        self.url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.mention_re = re.compile(r'@[A-Za-z0-9_]+')\n",
    "        self.hashtag_re = re.compile(r'#([A-Za-z0-9_]+)')\n",
    "        self.non_alpha_re = re.compile(r'[^a-z\\s]')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        cleaned = []\n",
    "        for text in X.astype(str):\n",
    "            t = text.lower()\n",
    "            t = self.url_re.sub(' ', t)\n",
    "            t = self.mention_re.sub(' ', t)\n",
    "            t = self.hashtag_re.sub(' \\1 ', t)  # keep the word, drop '#'\n",
    "            t = self.non_alpha_re.sub(' ', t)\n",
    "            t = re.sub(r'\\s+', ' ', t).strip()\n",
    "            cleaned.append(t)\n",
    "        return pd.Series(cleaned)\n",
    "\n",
    "# Quick smoke test\n",
    "_ = SimpleCleaner().fit_transform(pd.Series([\"Check this out: https://x.y\", \"@user #HappyDay!!!\"])).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551007c",
   "metadata": {},
   "source": [
    "## 4. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['target'], test_size=0.2, random_state=42, stratify=df['target']\n",
    ")\n",
    "len(X_train), len(X_test), y_train.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dde1bd",
   "metadata": {},
   "source": [
    "## 5. Vectorize + Logistic Regression (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efca56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple pipeline: Clean -> CountVectorizer -> LogisticRegression\n",
    "pipe = Pipeline([\n",
    "    ('clean', SimpleCleaner()),\n",
    "    ('vec', CountVectorizer(min_df=2, max_df=0.9, ngram_range=(1,2), stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, n_jobs=None))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "print(classification_report(y_test, pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81871aed",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57813692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred, labels=[0,1])\n",
    "df_cm = pd.DataFrame(cm, index=['neg(0)','pos(1)'], columns=['neg(0)','pos(1)'])\n",
    "print(df_cm)\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(df_cm, annot=True, fmt='d', cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa8376",
   "metadata": {},
   "source": [
    "## 7. Inference examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Life is a journey, enjoy it.\",\n",
    "    \"Mondays are the worst...\",\n",
    "    \"I am so happy with this new phone!\",\n",
    "    \"This service is terrible.\",\n",
    "    \"Absolutely fantastic performance tonight!\",\n",
    "    \"I regret buying this.\"\n",
    "]\n",
    "pred_ex = pipe.predict(pd.Series(examples))\n",
    "for t, y in zip(examples, pred_ex):\n",
    "    print(f\"{t}  ->  {'positive' if y==1 else 'negative'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499522d8",
   "metadata": {},
   "source": [
    "## 8. Save artifacts (optional)\n",
    "This lets you load and use the model later from a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca602c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe, '/mnt/data/logreg_text_clf.joblib')\n",
    "print(\"Saved model to /mnt/data/logreg_text_clf.joblib\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
